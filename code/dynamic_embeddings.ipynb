{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_hidden=0, num_layers=1, max_len_char=140, batch_mode=off, hidden_size=250, chars=no_numeric_upper, l2=0.000000, dropout_rate=0.500000, filter_lengths=[6, 7, 8], nb_filters=100, epochs=15, batch_size=50, optimizer=adadelta\n"
     ]
    }
   ],
   "source": [
    "# Settings for our network\n",
    "embedding_size = 200\n",
    "num_hidden = 0\n",
    "num_layers = 1\n",
    "hidden_size = 250\n",
    "l2 = 0\n",
    "dropout_rate = 0.5\n",
    "filter_lengths = [6, 7, 8]\n",
    "nb_filters = 100\n",
    "max_len_char = 140\n",
    "epochs = 15\n",
    "batch_mode = 'off'\n",
    "optimizer = 'adadelta'\n",
    "chars = 'no_numeric_upper'\n",
    "batch_size = 50\n",
    "\n",
    "parameters = 'num_hidden=%d, num_layers=%d, max_len_char=%d, batch_mode=%s, hidden_size=%d, chars=%s, l2=%f, dropout_rate=%f, filter_lengths=%s, nb_filters=%d, epochs=%d, batch_size=%d, optimizer=%s'\\\n",
    "                         % (num_hidden, num_layers, max_len_char, batch_mode, hidden_size, chars, l2, dropout_rate, str(filter_lengths), nb_filters, epochs, batch_size, optimizer)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1004399 tweets from 4391 unique users.\n",
      "Loading Twitter dataset took 2 seconds.\n",
      "Number of Tweets: 97728\n",
      "Only keeping characters that appear at least 100 times in the corpus\n",
      "Character set consists of 246 characters\n",
      "Building X...\n",
      "Building Y...\n",
      "Splitting Data...\n",
      "79159 train char sequences\n",
      "9773 test char sequences\n",
      "8796 validation char sequences\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "data = load_10_people()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = data['X_train'], data['Y_train'], data['X_val'], data['Y_val'],\\\n",
    "                                                data['X_test'], data['Y_test']\n",
    "    \n",
    "X_train = np.argmax(X_train, -1)\n",
    "X_val = np.argmax(X_val, -1)\n",
    "X_test = np.argmax(X_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model with dynamic embeddings\n",
    "from keras.layers import InputLayer, Convolution1D, MaxPooling1D, Concatenate, Flatten, Dense, Dropout, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dynamic embeddings and more n-grams\n",
    "input_layer = (Input(name = 'input', shape=(max_len_char,)))\n",
    "\n",
    "# Dynamic embeddings: TensorShape([Dimension(None), Dimension(246), Dimension(140)])\n",
    "embed = Embedding(input_dim=246, output_dim=140)(input_layer)\n",
    "\n",
    "convs = []\n",
    "for i in range(num_layers):\n",
    "    for ksize in [2,3,4,5,6]:\n",
    "        conv = (Convolution1D(filters=nb_filters, kernel_size=ksize, padding=\"valid\", activation=\"relu\",\\\n",
    "                                                 strides=1, name ='conv%d_%d' % (i, ksize))(embed))\n",
    "        pool = MaxPooling1D(pool_size =max_len_char - ksize + 1, name='pool%d_%d' % (i, ksize))(conv)\n",
    "        convs.append(pool)\n",
    "        \n",
    "concat = Concatenate()(convs)\n",
    "flatten = Flatten()(concat)\n",
    "flatten.get_shape()\n",
    "\n",
    "hidden = Dense(hidden_size, activation=\"relu\")(flatten)\n",
    "dropout = Dropout(rate=dropout_rate)(hidden)\n",
    "\n",
    "output = Dense(10, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79159 samples, validate on 8796 samples\n",
      "Epoch 1/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8580Epoch 00000: val_acc improved from -inf to 0.94509, saving model to ../models/dynamic_m2-00-0.95.hdf5\n",
      "79159/79159 [==============================] - 44s - loss: 0.4180 - acc: 0.8580 - val_loss: 0.1738 - val_acc: 0.9451\n",
      "Epoch 2/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9469Epoch 00001: val_acc improved from 0.94509 to 0.95418, saving model to ../models/dynamic_m2-01-0.95.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.1608 - acc: 0.9469 - val_loss: 0.1343 - val_acc: 0.9542\n",
      "Epoch 3/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9600Epoch 00002: val_acc improved from 0.95418 to 0.96510, saving model to ../models/dynamic_m2-02-0.97.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.1204 - acc: 0.9600 - val_loss: 0.1035 - val_acc: 0.9651\n",
      "Epoch 4/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9675Epoch 00003: val_acc improved from 0.96510 to 0.96567, saving model to ../models/dynamic_m2-03-0.97.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.0964 - acc: 0.9675 - val_loss: 0.1016 - val_acc: 0.9657\n",
      "Epoch 5/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9729Epoch 00004: val_acc did not improve\n",
      "79159/79159 [==============================] - 43s - loss: 0.0798 - acc: 0.9729 - val_loss: 0.1343 - val_acc: 0.9588\n",
      "Epoch 6/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9770Epoch 00005: val_acc improved from 0.96567 to 0.97135, saving model to ../models/dynamic_m2-05-0.97.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.0682 - acc: 0.9770 - val_loss: 0.0909 - val_acc: 0.9714\n",
      "Epoch 7/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9808Epoch 00006: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0562 - acc: 0.9808 - val_loss: 0.1384 - val_acc: 0.9592\n",
      "Epoch 8/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9832Epoch 00007: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0486 - acc: 0.9832 - val_loss: 0.0997 - val_acc: 0.9695\n",
      "Epoch 9/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9856Epoch 00008: val_acc improved from 0.97135 to 0.97260, saving model to ../models/dynamic_m2-08-0.97.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.0411 - acc: 0.9856 - val_loss: 0.0893 - val_acc: 0.9726\n",
      "Epoch 10/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9879Epoch 00009: val_acc improved from 0.97260 to 0.97499, saving model to ../models/dynamic_m2-09-0.97.hdf5\n",
      "79159/79159 [==============================] - 42s - loss: 0.0351 - acc: 0.9879 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 11/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9898Epoch 00010: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0292 - acc: 0.9898 - val_loss: 0.0939 - val_acc: 0.9746\n",
      "Epoch 12/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9915Epoch 00011: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0250 - acc: 0.9915 - val_loss: 0.0961 - val_acc: 0.9736\n",
      "Epoch 13/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9929Epoch 00012: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0208 - acc: 0.9929 - val_loss: 0.0970 - val_acc: 0.9744\n",
      "Epoch 14/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9944Epoch 00013: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0167 - acc: 0.9944 - val_loss: 0.1820 - val_acc: 0.9635\n",
      "Epoch 15/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9949Epoch 00014: val_acc did not improve\n",
      "79159/79159 [==============================] - 42s - loss: 0.0149 - acc: 0.9949 - val_loss: 0.1057 - val_acc: 0.9742\n"
     ]
    }
   ],
   "source": [
    "filepath=\"../models/dynamic_m2-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=[X_val, Y_val],\\\n",
    "         callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
