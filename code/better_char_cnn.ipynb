{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better Char CNN based on the code used in the paper by Ruder et. al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = import_dataset()\n",
    "\n",
    "print('Loading Twitter dataset took %d seconds.' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings for our network\n",
    "embedding_size = 200\n",
    "num_hidden = 0\n",
    "num_layers = 1\n",
    "hidden_size = 250\n",
    "l2 = 0\n",
    "dropout_rate = 0.5\n",
    "filter_lengths = [6, 7, 8]\n",
    "nb_filters = 100\n",
    "max_len_char = 140\n",
    "epochs = 15\n",
    "batch_mode = 'off'\n",
    "optimizer = 'adadelta'\n",
    "chars = 'no_numeric_upper'\n",
    "batch_size = 50\n",
    "\n",
    "parameters = 'num_hidden=%d, num_layers=%d, max_len_char=%d, batch_mode=%s, hidden_size=%d, chars=%s, l2=%f, dropout_rate=%f, filter_lengths=%s, nb_filters=%d, epochs=%d, batch_size=%d, optimizer=%s'\\\n",
    "                         % (num_hidden, num_layers, max_len_char, batch_mode, hidden_size, chars, l2, dropout_rate, str(filter_lengths), nb_filters, epochs, batch_size, optimizer)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the character set\n",
    "chars_set = load_charset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for every document, we have a feature matrix of **shape max_len_char x number of chars** (number of features) over which we convolve\n",
    "\n",
    "\n",
    "for all documents, we thus have a 3d tensor\n",
    "\n",
    "\n",
    "if we have a lot of documents, this tensor becomes too big (numpy arrays can be max 2GB in size, no matter the memory)\n",
    "we first try to create this array, if this fails, we default to batch_mode where we create the tensor only\n",
    "for the current mini-batch; this takes longer, but at least we can process an infinite amount of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only keep the top 10 most frequent authors to work with\n",
    "top10_authors = np.array(data.author.value_counts().index[:10])\n",
    "top10_authors\n",
    "\n",
    "top10_authors_data = data[data.author.isin(top10_authors)]\n",
    "print(\"Number of Tweets: {}\".format(len(top10_authors_data)))\n",
    "top10_authors_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only keep characters that appear at least 100 times in the corpus\n",
    "small_chars_set =  dict(filter(lambda x: x[1]>=100, chars_set.items()))\n",
    "small_char_indices = dict((c, i) for i, c in enumerate(small_chars_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_char = np.zeros((len(top10_authors_data), max_len_char, len(small_chars_set)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we only use characters as input channel, apply convolutions directly on one-hot character matrix without embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the data matrix with the OHE stuff\n",
    "# padding is incorporated in the process by letting it be 0\n",
    "\n",
    "# building X\n",
    "for doc_num, doc in enumerate(top10_authors_data.text):\n",
    "    for char_num, char in enumerate(doc):\n",
    "        # how to deal with docs, just keep part of the document             \n",
    "        if char_num >= max_len_char:\n",
    "            break\n",
    "        # unknown characters and padding are all mapped to the 0 vector\n",
    "        if char in small_char_indices:\n",
    "                X_char[doc_num, char_num, small_char_indices[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building Y\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "le = LabelEncoder()\n",
    "Y = ohe.fit_transform(le.fit_transform(top10_authors_data.author.values).reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train-Validation-Test split\n",
    "# Test is 90% of original\n",
    "# Val is 10% of train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_char, X_test_char, Y_train, Y_test = train_test_split(X_char, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "X_train_char, X_val_char, Y_train, Y_val = train_test_split(X_train_char, Y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('%d train char sequences' % len(X_train_char))\n",
    "print('%d test char sequences' % len(X_test_char))\n",
    "print('%d validation char sequences' % len(X_val_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset some unused variables\n",
    "X_char = None\n",
    "Y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer, Convolution1D, MaxPooling1D, Concatenate, Flatten, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "input_layer = (Input(name = 'input', shape=(max_len_char, len(small_chars_set))))\n",
    "\n",
    "conv1 = Convolution1D(filters=nb_filters, kernel_size=j, padding=\"valid\", activation=\"relu\",\\\n",
    "                                         strides=1, name ='conv%d_%d' % (i, j))(input_layer)\n",
    "convs = []\n",
    "for i in range(num_layers):\n",
    "    for j in filter_lengths:\n",
    "        conv = (Convolution1D(filters=nb_filters, kernel_size=j, padding=\"valid\", activation=\"relu\",\\\n",
    "                                         strides=1, name ='conv%d_%d' % (i, j))(input_layer))\n",
    "        pool = MaxPooling1D(pool_size =max_len_char - j + 1, name='pool%d_%d' % (i, j))(conv)\n",
    "        convs.append(pool)\n",
    "        \n",
    "concat = Concatenate()(convs)\n",
    "flatten = Flatten()(concat)\n",
    "flatten.get_shape()\n",
    "\n",
    "hidden = Dense(hidden_size, activation=\"relu\")(flatten)\n",
    "dropout = Dropout(rate=dropout_rate)(hidden)\n",
    "\n",
    "output = Dense(10, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath=\"../models/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "hist = model.fit(X_train_char, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val_char, Y_val),\\\n",
    "         callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.load_weights(\"weights.best.hdf5\")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
