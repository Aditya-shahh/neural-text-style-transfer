{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better Char CNN based on the code used in the paper by Ruder et. al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_hidden=0, num_layers=1, max_len_char=140, batch_mode=off, hidden_size=250, chars=no_numeric_upper, l2=0.000000, dropout_rate=0.500000, filter_lengths=[6, 7, 8], nb_filters=100, epochs=15, batch_size=50, optimizer=adadelta\n"
     ]
    }
   ],
   "source": [
    "# Settings for our network\n",
    "embedding_size = 200\n",
    "num_hidden = 0\n",
    "num_layers = 1\n",
    "hidden_size = 250\n",
    "l2 = 0\n",
    "dropout_rate = 0.5\n",
    "filter_lengths = [6, 7, 8]\n",
    "nb_filters = 100\n",
    "max_len_char = 140\n",
    "epochs = 15\n",
    "batch_mode = 'off'\n",
    "optimizer = 'adadelta'\n",
    "chars = 'no_numeric_upper'\n",
    "batch_size = 50\n",
    "\n",
    "parameters = 'num_hidden=%d, num_layers=%d, max_len_char=%d, batch_mode=%s, hidden_size=%d, chars=%s, l2=%f, dropout_rate=%f, filter_lengths=%s, nb_filters=%d, epochs=%d, batch_size=%d, optimizer=%s'\\\n",
    "                         % (num_hidden, num_layers, max_len_char, batch_mode, hidden_size, chars, l2, dropout_rate, str(filter_lengths), nb_filters, epochs, batch_size, optimizer)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1004399 tweets from 4391 unique users.\n",
      "Loading Twitter dataset took 3 seconds.\n",
      "Number of Tweets: 97728\n",
      "Only keeping characters that appear at least 100 times in the corpus\n",
      "Character set consists of 246 characters\n",
      "Building X...\n",
      "Building Y...\n",
      "Splitting Data...\n",
      "79159 train char sequences\n",
      "9773 test char sequences\n",
      "8796 validation char sequences\n"
     ]
    }
   ],
   "source": [
    "data = load_10_people()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = data['X_train'], data['Y_train'], data['X_val'], data['Y_val'],\\\n",
    "                                                data['X_test'], data['Y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79159 samples, validate on 8796 samples\n",
      "Epoch 1/15\n",
      "79150/79159 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.7413Epoch 00001: val_acc improved from -inf to 0.89995, saving model to ../models/ruder-01-0.90.hdf5\n",
      "79159/79159 [==============================] - 961s 12ms/step - loss: 0.7691 - acc: 0.7413 - val_loss: 0.2988 - val_acc: 0.9000\n",
      "Epoch 2/15\n",
      "15900/79159 [=====>........................] - ETA: 13:24 - loss: 0.3058 - acc: 0.9014"
     ]
    }
   ],
   "source": [
    "from keras.layers import InputLayer, Convolution1D, MaxPooling1D, Concatenate, Flatten, Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "input_layer = (Input(name = 'input', shape=(max_len_char, 246)))#len(small_chars_set))))\n",
    "\n",
    "convs = []\n",
    "for i in range(num_layers):\n",
    "    for j in filter_lengths:\n",
    "        conv = (Convolution1D(filters=nb_filters, kernel_size=j, padding=\"valid\", activation=\"relu\",\\\n",
    "                                         strides=1, name ='conv%d_%d' % (i, j))(input_layer))\n",
    "        pool = MaxPooling1D(pool_size =max_len_char - j + 1, name='pool%d_%d' % (i, j))(conv)\n",
    "        convs.append(pool)\n",
    "        \n",
    "concat = Concatenate()(convs)\n",
    "flatten = Flatten()(concat)\n",
    "flatten.get_shape()\n",
    "\n",
    "hidden = Dense(hidden_size, activation=\"relu\")(flatten)\n",
    "dropout = Dropout(rate=dropout_rate)(hidden)\n",
    "\n",
    "output = Dense(10, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath=\"../models/ruder-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=[X_val, Y_val],\\\n",
    "         callbacks = callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
