{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from nn import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1004399 tweets from 4391 unique users.\n",
      "Loading Twitter dataset took 2 seconds.\n",
      "Number of Tweets: 97728\n",
      "Only keeping characters that appear at least 100 times in the corpus\n",
      "Character set consists of 246 characters\n",
      "Building X...\n",
      "Building Y...\n",
      "Splitting Data...\n",
      "79159 train char sequences\n",
      "9773 test char sequences\n",
      "8796 validation char sequences\n"
     ]
    }
   ],
   "source": [
    "# import all the data\n",
    "data = load_10_people()\n",
    "X_train_ohe = data['X_train'].astype(np.float64)\n",
    "X_train_nums = X_train_ohe.argmax(-1)\n",
    "Y_train = data['Y_train'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the encoder RNN with one GRU cell\n",
    "vocab_size = 246\n",
    "embedding_size = 100\n",
    "max_seq_length = 140\n",
    "n_layers = 1\n",
    "batch_size = 50\n",
    "nepochs = 10\n",
    "dropout_rate = 0.5\n",
    "dim_y = 10\n",
    "dim_h = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gru_cell(dim, n_layers, dropout):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(dim)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "        input_keep_prob=dropout)\n",
    "    if n_layers > 1:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * n_layers)\n",
    "    return cell\n",
    "\n",
    "# fully-conected layer\n",
    "def dense(x, inputFeatures, outputFeatures, scope):\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [inputFeatures, outputFeatures],\n",
    "                                 tf.float32, tf.random_normal_initializer(stddev=0.02))\n",
    "        bias = tf.get_variable(\"bias\", [outputFeatures], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        return tf.matmul(x, matrix) + bias\n",
    "    \n",
    "# take the encoded sentence and try to predict mu and sigma\n",
    "def recognition(latent_vector, dim_mu = dim_h, dim_s = dim_h):\n",
    "        with tf.variable_scope(\"recognition\"):\n",
    "            w_mean = dense(latent_vector, dim_h, dim_mu, \"w_mean\")\n",
    "            w_stddev = dense(latent_vector, dim_h, dim_s, \"w_stddev\")\n",
    "        return w_mean, w_stddev\n",
    "    \n",
    "def reconstruction_loss(targets, logits):\n",
    "    with tf.variable_scope(\"rec_loss\"):\n",
    "        loss_g = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.reshape(targets, [-1, vocab_size]), logits=logits)\n",
    "        loss_g = tf.reduce_sum(loss_g) / tf.to_float(batch_size)\n",
    "\n",
    "        preds = tf.argmax(tf.nn.softmax(logits), -1)\n",
    "        goalz = tf.cast(tf.reshape(encoder_inputs, [-1]), tf.int64)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, goalz), tf.float32))\n",
    "        \n",
    "    return loss_g, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Network - AAE with adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator(latent_encoding, labels):\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        logits = dense(latent_encoding, dim_h, 10, \"discriminator\")\n",
    "        disc_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits))\n",
    "        \n",
    "        preds = tf.argmax(tf.nn.softmax(logits), -1)\n",
    "        goalz = tf.argmax(labels, -1)\n",
    "        disc_acc = tf.reduce_mean(tf.cast(tf.equal(preds, goalz), tf.float32))\n",
    "        \n",
    "    return disc_loss, disc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "encoder_inputs = tf.placeholder(shape = [batch_size, max_seq_length], name = \"input_sentences\", dtype=tf.int32)\n",
    "targets = tf.placeholder(shape = [batch_size, max_seq_length, vocab_size], name = \"target_sentences\", dtype=tf.int32)\n",
    "labels = tf.placeholder(shape = [batch_size], name = 'labels', dtype=tf.int32)\n",
    "labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "# Embedding matrices\n",
    "# TensorShape([Dimension(246), Dimension(100)])\n",
    "embedding_encoder = tf.get_variable(\"embedding_matrix\", [vocab_size, embedding_size])\n",
    "embedding_decoder = tf.get_variable(\"output_embedding_matrix\", [vocab_size, embedding_size])\n",
    "\n",
    "# Extract Embeddings\n",
    "#TensorShape([Dimension(140), Dimension(50), Dimension(100)])\n",
    "encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs, name = \"encoder_embeddings\")\n",
    "encoder_emb_out = tf.nn.embedding_lookup(embedding_decoder, encoder_inputs, name = \"decoder_embeddings\")\n",
    "\n",
    "# Encoder Net\n",
    "# dim_h is the dimension of the hidden state\n",
    "cell_e = create_cell(dim_h, n_layers, dropout_rate)\n",
    "_, z = tf.nn.dynamic_rnn(cell_e, encoder_emb_inp, dtype = tf.float32, scope='encoder')\n",
    "\n",
    "# Decoder Net\n",
    "cell_g = create_cell(dim_h, n_layers, dropout_rate)\n",
    "g_outputs, _ = tf.nn.dynamic_rnn(cell_g, encoder_emb_out,\n",
    "            initial_state = z, scope='generator')\n",
    "g_outputs = tf.nn.dropout(g_outputs, dropout_rate)\n",
    "# flatten all the outputs and take through final FC layer for classification\n",
    "g_outputs = tf.reshape(g_outputs, [-1, dim_h])\n",
    "g_logits = dense(g_outputs, dim_h, vocab_size, scope='output_fc')\n",
    "\n",
    "# Losses\n",
    "disc_loss, disc_acc = discriminator(z,tf.reshape(tf.one_hot(labels, 10),[batch_size, -1]) )\n",
    "rec_loss, rec_acc = reconstruction_loss(targets, g_logits)\n",
    "rho = 0.3\n",
    "total_loss = rec_loss + rho*disc_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:01<2:14:35,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicriminator Accuracy 0.18000000715255737\n",
      "Loss: 483.89935302734375\n",
      "Accuracy 0.01842857152223587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 201/5000 [03:12<1:26:50,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicriminator Accuracy 0.2199999988079071\n",
      "Loss: 256.6048583984375\n",
      "Accuracy 0.14585714042186737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 401/5000 [06:36<1:22:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicriminator Accuracy 0.41999998688697815\n",
      "Loss: 205.06509399414062\n",
      "Accuracy 0.3107142746448517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 522/5000 [08:35<1:11:18,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run([tf.global_variables_initializer()])\n",
    "\n",
    "max_iter = 5000\n",
    "summary_iter = 200\n",
    "\n",
    "rec_accuracy_ = []\n",
    "loss_ = []\n",
    "disc_accuracy_ = []\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "        sess.run([optimizer], feed_dict={encoder_inputs: X_train_nums[i:i+batch_size], \n",
    "                                                       targets: X_train_ohe[i:i+batch_size],\n",
    "                                                       labels: np.argmax(Y_train[i:i+batch_size], 1)})\n",
    "        if i % summary_iter == 0:\n",
    "            random_ix = np.random.choice(np.arange(len(X_train_nums)),size = batch_size,replace=True)\n",
    "            l, a, disc_a = sess.run([total_loss, rec_acc, disc_acc], feed_dict={encoder_inputs: X_train_nums[random_ix], \n",
    "                                                       targets: X_train_ohe[random_ix],\n",
    "                                                       labels: np.argmax(Y_train[random_ix], 1)})\n",
    "            \n",
    "            # Get discriminator's accuracy\n",
    "            print(\"Dicriminator Accuracy {}\".format(disc_a))\n",
    "            \n",
    "            # Get total loss and reconstruction Accuracy\n",
    "            print(\"Loss: {}\".format(l))\n",
    "            print(\"Accuracy {}\".format(a))\n",
    "            \n",
    "            rec_accuracy_.append(a)\n",
    "            loss_.append(l)\n",
    "            disc_accuracy_.append(disc_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
