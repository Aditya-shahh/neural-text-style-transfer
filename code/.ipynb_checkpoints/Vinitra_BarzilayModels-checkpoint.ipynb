{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1004399 tweets from 4391 unique users.\n",
      "Loading Twitter dataset took 3 seconds.\n",
      "Number of Tweets: 97728\n",
      "Only keeping characters that appear at least 100 times in the corpus\n",
      "Character set consists of 246 characters\n",
      "Building X...\n",
      "Building Y...\n",
      "Splitting Data...\n",
      "79159 train char sequences\n",
      "9773 test char sequences\n",
      "8796 validation char sequences\n"
     ]
    }
   ],
   "source": [
    "data = load_10_people()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = data['X_train'], data['Y_train'], data['X_val'], data['Y_val'],\\\n",
    "                                                data['X_test'], data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79159, 10), (79159, 140, 246))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1004399 tweets from 4391 unique users.\n",
      "Number of Tweets: 43164\n"
     ]
    }
   ],
   "source": [
    "data = import_dataset()\n",
    "top2_authors = np.array(data.author.value_counts().index[:2])\n",
    "top2_authors_data = data[data.author.isin(top2_authors)]\n",
    "print(\"Number of Tweets: {}\".format(len(top2_authors_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@ StephanieQuayle Thanks for the # cyberhug for Patricia . We 'll forward your # hugandkudos on to her .\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "def clean(tweet):\n",
    "#     punctuation = list(string.punctuation)\n",
    "    return ' '.join(word_tokenize(tweet))\n",
    "#     for punc in punctuation:\n",
    "#         p_loc = tweet.find(punc)\n",
    "#         if p_loc != -1:\n",
    "#             if tweet[p_loc+1] == \" \":\n",
    "#                 tweet = tweet.replace(punc, \" \"+punc)\n",
    "#             elif p_loc-1 > 0 and tweet[p_loc-1]==\" \":\n",
    "#                 tweet = tweet.replace(punc, punc+\" \")\n",
    "clean(\"@StephanieQuayle Thanks for the #cyberhug for Patricia. We'll forward your #hugandkudos on to her.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ \"@ StephanieQuayle Thanks for the # cyberhug for Patricia . We 'll forward your # hugandkudos on to her .\",\n",
       "        \"@ mholzke Link is perfect ! We 'll share your kind words to the ORD leadership team . Have an amazing day .\",\n",
       "        \"@ Chic_Wood We know you do n't want to miss your flight , but we ca n't make any promises .\",\n",
       "        ...,\n",
       "        \"@ mra99 We do n't like hearing that , Michael . We expect our team to be kind . What 's your flight number and how can we help ?\",\n",
       "        '@ __XT Your bags should transfer with you and be waiting for you in DCA . We appreciate your patience .',\n",
       "        \"@ mbirney What 's your flight number ?\"],\n",
       "       dtype='<U157'),\n",
       " array([ '# NowPlaying Built To Spill - Some Things Last A Long Time ** https : //t.co/daLPPfw73z , Tune-in and enjoy',\n",
       "        '# NowPlaying Pulp - We Are The Boyz ** https : //t.co/daLPPfw73z , Tune-in and enjoy',\n",
       "        \"# NowPlaying Paul Christopher - What 's The Use Of Trying ** https : //t.co/daLPPfw73z , Tune-in and enjoy\",\n",
       "        ...,\n",
       "        \"# NowPlaying Sasser - What 's Up G ** https : //t.co/daLPPfw73z , Tune-in and enjoy\",\n",
       "        '# NowPlaying Sasser - Pretty Little Rich Girl ** https : //t.co/daLPPfw73z , Tune-in and enjoy',\n",
       "        '# NowPlaying Aronora - Set To Fail ** https : //t.co/daLPPfw73z , Tune-in and enjoy'],\n",
       "       dtype='<U155'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_1 = top2_authors[0]\n",
    "author_2 = top2_authors[1]\n",
    "author1_data = np.array([clean(i) for i in data[data.author.isin([author_1])]['text']] )\n",
    "author2_data = np.array([clean(i) for i in data[data.author.isin([author_2])]['text']] )\n",
    "author1_data, author2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev0=author1_data[:400]\n",
    "test0=author1_data[400:800]\n",
    "train0=author1_data[800:1200]\n",
    "dev1=author2_data[:400]\n",
    "test1=author2_data[400:800]\n",
    "train1=author2_data[800:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41014"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('../language-style-transfer/data/tweets/author.dev.0', 'w')\n",
    "file.write(\"\\n\".join(dev0))\n",
    "file = open('../language-style-transfer/data/tweets/author.test.0', 'w')\n",
    "file.write(\"\\n\".join(test0))\n",
    "file = open('../language-style-transfer/data/tweets/author.train.0', 'w')\n",
    "file.write(\"\\n\".join(train0))\n",
    "file = open('../language-style-transfer/data/tweets/author.dev.1', 'w')\n",
    "file.write(\"\\n\".join(dev1))\n",
    "file = open('../language-style-transfer/data/tweets/author.test.1', 'w')\n",
    "file.write(\"\\n\".join(test1))\n",
    "file = open('../language-style-transfer/data/tweets/author.train.1', 'w')\n",
    "file.write(\"\\n\".join(train1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
