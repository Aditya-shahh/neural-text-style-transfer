{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 254\n",
    "embedding_size = 300\n",
    "tweet_size = 140\n",
    "batch_size = 100\n",
    "num_sampled = 50\n",
    "num_filters = 32\n",
    "num_classes_for_prediction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the input data\n",
    "twitter_data_dir = \"../data/authorship_corpora/influencer_tweets.json\"\n",
    "json_data=open(twitter_data_dir).read()\n",
    "twitter_data = json.loads(json_data)\n",
    "\n",
    "# input_matrix = [np.zeros((vocab_size, tweet_size)) for i in np.arange(num_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(254), Dimension(300)])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholders for inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size, tweet_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, num_classes_for_prediction])\n",
    "\n",
    "# one hot encode training labels\n",
    "\n",
    "# input: vocab * n chars of specific tweets\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embeddings.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(140), Dimension(300), Dimension(1)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "embed = tf.reshape(embed, [100, 140, 300, 1])\n",
    "embed.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer #1\n",
    "with tf.name_scope(\"conv-maxpool-1\"):\n",
    "    W = tf.Variable(tf.truncated_normal([2, embedding_size, 1, num_filters], stddev=0.05), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        embed, \n",
    "        W, \n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding=\"VALID\", \n",
    "        name=\"conv1\")\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1, 139, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(1), Dimension(1), Dimension(32)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(10)])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully connected layer 1\n",
    "h_pool_flat = tf.reshape(pooled, [batch_size, num_filters])\n",
    "\n",
    "# Multiply w * h_pool_flat and add bias terms\n",
    "# 100*32 * 32*10 -> 100*10 + 10*1 = fc_1_output\n",
    "with tf.name_scope(\"fc-1\"):\n",
    "    W = tf.Variable(tf.truncated_normal([num_filters, num_classes_for_prediction], stddev=0.05), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_classes_for_prediction]), name=\"b\")\n",
    "    fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b), name=\"fc-1-out\")\n",
    "\n",
    "fc_1_output.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross entropy loss function\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "    _sentinel=None,\n",
    "    labels=train_labels,\n",
    "    logits=fc_1_output,\n",
    "    dim=-1,\n",
    "    name=None\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-62306ff10d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "    feed_dict = {train_inputs: inputs, train_labels: labels}\n",
    "    _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize embeddings using TSNE\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
